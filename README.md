# Practice 3 - Due on 17th Nov
Xinran: netup function
Zheyue: forward and backward function
Bo: train function, train the network by training data and test the model

# Practice 2 - Due on 20th Oct
![91769fffd5413849932d0b61dc978b7](https://github.com/Michelle-LZY/Extended_Stat_Programming-2023/assets/136700489/3b6c5640-4e53-4ef6-b6db-398dab343a03)

# Practice 1 - Due on 6th Oct
<details>
  <summary>Click me to show details about Practice 1</summary>

### Group members: Bo Gao(s2511232), Zheyue Lin(s2519324) and Xinran Zhu(s2508695)
### Task 1: Create the group repo: Zheyue Lin
### Task 2-5: Bo Gao
- [x] Read txt file into R
- [x] Write a function 'split_punct', which takes a vector of words as input along with a punctuation mark
      Search for each word containing the punctuation mark, and remove it from the word, and add the mark as a new entry in the vector of words after the word it came from  
      This function will return the updated vector extract and then remove all the punctuations in the text and save the words vector as 'a'  
- [x] Use 'split_punct' function to seperate the punctuation marks

### Task 6-7: Zheyue Lin
- [x] Create a vector 'b' that hold the m most commonly occuring words. (m â‰ˆ 1000)
- [x] Make the matrices of common word triplets(T) and pairs(P).

### Task 8: Bo Gao, Xinran Zhu
- [x] Write code to simulate 50-word sections from your model.  
      Do this by using the model to simulate integers indexing words in vector 'b'.  
      Then print out the corresponding text with 'cat'.
      The 'sample' function should be used to select a word(index) with a given probability.

### Task 9: Bo Gao
- [x] For comparison, simulate 50 word sections of text where the word probabilities are simply

### Task 10: Zheyue Lin
- [x] Keep words having capital letter the most often in the original text.
</details>


