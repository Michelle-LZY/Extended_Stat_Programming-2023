b_nn_db <- b_nn$db
# if sum_dW and sum_db is empty,
# set sum_dW equal to b_nn_dW and sum_db equal to b_nn_db
#if(length(sum_db) == 0){
#  sum_dW <- b_nn_dW
#  sum_db <- b_nn_db
#}
# if sum_dW and sum_db is not empty,
# add b_nn_dW to sum_dW and add b_nn_db to sum_db
#else {
#  for (j in 1:length(b_nn_db)){
#    sum_dW[[j]] <- sum_dW[[j]] + b_nn_dW[[j]]
#    sum_db[[j]] <- sum_db[[j]] + b_nn_db[[j]]
#  }
for (j in 1:length(b_nn_db)){
sum_dW[[j]] <- sum_dW[[j]] + b_nn_dW[[j]]
sum_db[[j]] <- sum_db[[j]] + b_nn_db[[j]]
}
}
# update the new W equal to the old W minus eta*sum_dW/mb
# and new b equal to the old b minus eta*sum_dw/mb
# because the average value of dW is equal to sum_dW/mb
# and the average value of db is equal to sum_db/mb
nn$W <- mapply(function(x, y) x - eta*(y/mb), nn$W, sum_dW, SIMPLIFY = FALSE)
nn$b <- mapply(function(x, y) x - eta*(y/mb), nn$b, sum_db, SIMPLIFY = FALSE)
}
return (nn)
}
# we use the dataset 'iris' from R to train a 4-8-7-3 network
data(iris)
# find the class for species in 'iris'
class <- unique(iris[,"Species"])
# let different speices represent in different number
iris[,"Species"]<-as.numeric(iris[,"Species"])
# 'indices' is the rows for 'iris'
indices <- 1:nrow(iris)
# select the test data consists of every 5 rows
iris_test <- iris[indices %% 5 == 0,]
# select the train data consists of iris which are not in test data
iris_train <- iris[indices %% 5 != 0,]
# set the network structures and initialize it
layers <- c(4,8,7,3)
iris_nn <- netup(layers)
# train the network based on train dataset
iris_nn <- train(iris_nn, iris_train[, 1:4], iris_train[, 5])
l <- vector("list", 3)
l
l[[1]]<-l[[1]]+c(2,3,4)
l
l <- vector("list", 3)
l<-0
l
help(vector)
l <- vector("list", 3)
l<-l+0
lapply(1:num_sublists, function(_) numeric(0))
lapply(1:3, function(_) numeric(0))
lapply(1:3, function(x) list(0))
lapply(vector("list",3), function(x) list(0))
sapply(vector("list",3), function(x) list(0))
l<-sapply(vector("list",3), function(x) list(0))
l[[1]]<-c(1,2,3)+l[[1]]
l
# Group members: Bo Gao(s2511232), Zheyue Lin(s2519324) and Xinran Zhu(s2508695)
# https://github.com/Michelle-LZY/Extended_Stat_Programming-2023.git
# Xinran Zhu(s2508695) wrote netup() function.
# Zheyue Lin(s2519324) coded forward() and backward() functions, also revised the
# comments.
# Bo Gao(s2511232) worked on train() function and tested training data set.
# Briefly Introduction of the project:
# This project sets up a simple neural network for classification, and to train
# it using stochastic gradient descent.
# There are different layers in the network and on each layer there are nodes.
# Nodes of the network each contain values h^l_j.
# The values for the first layer nodes are set to the values of input data: inp.
# The network then combines and transforms the values in each layer to produce
# the values at the next layer, until we reach the output layer L. We wrote a
# forward function to compute the remaining node values implied by inp. The output
# layer nodes are used to predict output data (aka response data) associated with
# the input data.
# The network has parameters controlling the combinations and transformations
# linking each layer to the next.(matrix W) These parameters are adjusted in
# order to make the input data best predict the output data according to some loss
# function, L. This is known as training the network.
# Stochastic Gradient Descent is an optimization approach that helps to maintain
# a good generalizing power, without getting too fixated on fitting any particular
# set of data. The idea is to repeatedly find the gradient of the loss function
# w.r.t. the parameters for small randomly chosen subsets of the training data,
# and to adjust the parameters by taking a step in the direction of the negative
# gradient.
start_time <- Sys.time()
# Set seed to make the results more stable
set.seed(2)
# netup() is the first function to use, a function to return a list of vector
# containing:
# h: a list of nodes for each layer. h[[l]] is a vector of length d[l] which
# contains the node values for layer l.
# W: a list of weight matrices W[[l]] is the weight matrix linking layer l to
# layer l+1, so W[[l]] is a d[l+1]*d[l] matrix and the length of W list will be
# L-1 (L is the number of total layers). Initialize the elements with U(0, 0.2)
# random deviates.
# b: a list of offset vectors. b[[l]] is the offset vector linking layer l to
# layer l+1, so the length of b[[l]] is equal to d[l+1] and there are L-1 vectors
# in list b (L is the maximum layer). Initialize the elements with U(0, 0.2)
# random deviates.
netup <- function(d){
# "lapply" applies function (length){return(rep(0,length))} to each element in
# d. For example, h[[i]] is a zero vector and length(h[[i]]) = d[i]
h <- lapply(d, function(length) rep(0,length))
W <- list()
b <- list()
# The number of layers
L <- length(d)
# There are only L-1 W matrices and L-1 b vectors
for (i in 1:(L-1)){
# Initialize the size of 'W' and W[[i]] is a d[i+1]*d[i] matrix
# Fill in the W matrices with uniformly distributed numbers within 0 and 0.2
W[[i]] <- matrix(runif(d[i] * d[i+1], 0, 0.2), d[i+1], d[i])
# Same idea to initialize b
b[[i]] <- runif(d[i+1], 0, 0.2)
}
# Return initialized network list
return(list("h" = h, "W" = W, "b" = b))
}
# forward() is the second function to use, compute the remaining node values
# implied by inp, and return the updated network list (as the only return object).
# nn: a network list as returned by netup
# inp: a vector of input values for the first layer.
# This forward function should
forward <- function(nn, inp){
# "netup_" marks network list returned by function netup
update_h <- nn$h
netup_W <- nn$W
netup_b <- nn$b
# Fill in updated h^1 with inp
update_h[[1]] <- inp
# The number of layers
netup_L <- length(update_h)
for(l in 1:(netup_L - 1)){
# For layer l, Whb = W %*% h + b. Whb is a vector.
# W[[l]] is the matrix linking l and l+1
# h[[l]] is a nodes vector
# b[[l]] is an offset vector
# with length(Whb^l) = length(h^(l+1))
Whb <- netup_W[[l]] %*% update_h[[l]] + netup_b[[l]]
# For h in layer l+1, h_j = max(Whb_j, 0)
for(i in 1:length(Whb)){
if(Whb[i] > 0){
update_h[[l+1]][i] <- Whb[i]
}
}
}
# Return updated network list
networklist <- list("h" = update_h, "W" = netup_W, "b" = netup_b)
return(networklist)
}
# Write a function backward(nn,k) for computing the derivatives of the loss
# corresponding to output class k for network nn (returned from forward function).
# Derivatives w.r.t. the nodes, weights and offsets should be computed and added
# to the network list as lists dh, dw, db.
# nn: network returned from forward function
# k: an integer representing class
backward<-function(nn, k){
# "f_" marks h, W and b returned from forward function.
# The length of lists h, W, b are the same as explained above for netup function
f_h <- nn$h ## length: f_L
f_W <- nn$W ## length: f_L - 1
f_b <- nn$b ## length: f_L - 1
f_L <- length(f_h) ## The number of layers
# Calculate the derivative of the loss k w.r.t. nodes on the last layer L
# Create an empty list having L sublists
dh <- vector("list", f_L)
# expj is a vector that expj[j] = exp(hL[j]), where f_h[[f_L]] = hL, the nodes
# vector on the last layer L
expj <- exp(f_h[[f_L]])
# Sum exp(hL[q]) for all q in hL
sumq <- sum(expj)
# dh_L[j] = exp(h_L)[j]/sum(h_L[q]), if j is not equal to k
dh[[f_L]] <- expj/sumq
# dh_L[k] = exp(h_L)[k]/sum(h_L[q])
dh[[f_L]][k] <- expj[k]/sumq - 1
# Compute derivatives of L w.r.t all other nodes by working backwards through
# the layers applying the chain rule (back-propagation)
db <- vector("list", f_L - 1) ## length: f_L - 1
dW <- vector("list", f_L - 1) ## length: f_L - 1
for (l in (f_L-1):1){
# For layer l
# If h(l+1)[j] is positive, d[j] = dh(l+1)[j]
d <- dh[[l+1]]
# If h(l+1)[j] is negative, d[j] = 0
d[which(f_h[[l+1]] < 0)] <- 0
dh[[l]] <- t(f_W[[l]]) %*% d
db[[l]] <- d
dW[[l]] <- d %*% t(f_h[[l]])
}
# Update the network lists
network = list("h" = f_h, "W" = f_W, "b" = f_b, "dh" = dh, "dW" = dW, "db" = db)
return (network)
}
# train function combined forward and backward functions to find the optimal
# value of W and b to get the best result for the current tasks and return the
# updated network list
# nn: a network list as returned by netup function
# inp: a vector of input values for the first layer.
# k: an integer representing class
# eta: a float representing step size
# mb: an integer representing the number of data to randomly sample
# nstep: an integer representing the the number of optimization steps
train <- function(nn, inp, k, eta = .01, mb = 10, nstep = 10000){
L <- length(nn$h)
# Iterative nstep times to train W and b
for (istep in 1:nstep){
# Randomly choose mb rows as first layer values inp
s <- sample(nrow(inp), mb)
# Define sum_dW and sum_db to store the sum of derivatives dW and db
sum_dW <- sapply(vector("list", L-1), function(x) list(0))
sum_db <- sapply(vector("list", L-1), function(x) list(0))
# Iterative the randomly chose first layer inputs to get the different dW
# and db values, sum them up, then get the average values of dW and db
for (i in s){
# Use forward function to update all nodes in h list
f_nn <- forward(nn, as.numeric(inp[i,]))
# Use backward function to get dW and db
b_nn <- backward(f_nn, k[i])
# b_nn_dW and b_nn_db are used to store updated dW and db
b_nn_dW <- b_nn$dW
b_nn_db <- b_nn$db
# if sum_dW and sum_db is empty,
# set sum_dW equal to b_nn_dW and sum_db equal to b_nn_db
#if(length(sum_db) == 0){
#  sum_dW <- b_nn_dW
#  sum_db <- b_nn_db
#}
# if sum_dW and sum_db is not empty,
# add b_nn_dW to sum_dW and add b_nn_db to sum_db
#else {
#  for (j in 1:length(b_nn_db)){
#    sum_dW[[j]] <- sum_dW[[j]] + b_nn_dW[[j]]
#    sum_db[[j]] <- sum_db[[j]] + b_nn_db[[j]]
#  }
for (j in 1:length(b_nn_db)){
sum_dW[[j]] <- sum_dW[[j]] + b_nn_dW[[j]]
sum_db[[j]] <- sum_db[[j]] + b_nn_db[[j]]
}
}
# update the new W equal to the old W minus eta*sum_dW/mb
# and new b equal to the old b minus eta*sum_dw/mb
# because the average value of dW is equal to sum_dW/mb
# and the average value of db is equal to sum_db/mb
nn$W <- mapply(function(x, y) x - eta*(y/mb), nn$W, sum_dW, SIMPLIFY = FALSE)
nn$b <- mapply(function(x, y) x - eta*(y/mb), nn$b, sum_db, SIMPLIFY = FALSE)
}
return (nn)
}
# we use the dataset 'iris' from R to train a 4-8-7-3 network
data(iris)
# find the class for species in 'iris'
class <- unique(iris[,"Species"])
# let different speices represent in different number
iris[,"Species"]<-as.numeric(iris[,"Species"])
# 'indices' is the rows for 'iris'
indices <- 1:nrow(iris)
# select the test data consists of every 5 rows
iris_test <- iris[indices %% 5 == 0,]
# select the train data consists of iris which are not in test data
iris_train <- iris[indices %% 5 != 0,]
# set the network structures and initialize it
layers <- c(4,8,7,3)
iris_nn <- netup(layers)
# train the network based on train dataset
iris_nn <- train(iris_nn, iris_train[, 1:4], iris_train[, 5])
# define misclassification to store the number of misclassification
misclassification <- 0
# look through each row in test data
for (i in 1:nrow(iris_test)){
# compute the remaining node values implied by the first four elements
# for each row in test data and update the network list
i_nn <- forward(iris_nn, as.numeric(iris_test[i, 1:4]))
# find the maximum in the last layers, which is the predicted class
i_class <- which.max(i_nn$h[[length(layers)]])
# if the predicted class is equal to the label, it is true
# but if the result is different from the label, it means it is wrong
# so the number of misclassification should be plus 1
if (i_class != iris_test[i, 5]){
misclassification <- misclassification + 1
}
}
end_time <- Sys.time()
print(end_time - start_time)
# Calculate the misclassification rate (i.e. the proportion misclassified) for the
# test set.
misclassification_rate <- misclassification/nrow(iris_test)
print(misclassification_rate)
l[[2]] += c(2,3,4)
iris[,"Species"]
iris[,"Species"]
# Group members: Bo Gao(s2511232), Zheyue Lin(s2519324) and Xinran Zhu(s2508695)
# https://github.com/Michelle-LZY/Extended_Stat_Programming-2023.git
# Xinran Zhu(s2508695) wrote netup() function.
# Zheyue Lin(s2519324) coded forward() and backward() functions, also revised the
# comments.
# Bo Gao(s2511232) worked on train() function and tested training data set.
# Briefly Introduction of the project:
# This project sets up a simple neural network for classification, and to train
# it using stochastic gradient descent.
# There are different layers in the network and on each layer there are nodes.
# Nodes of the network each contain values h^l_j.
# The values for the first layer nodes are set to the values of input data: inp.
# The network then combines and transforms the values in each layer to produce
# the values at the next layer, until we reach the output layer L. We wrote a
# forward function to compute the remaining node values implied by inp. The output
# layer nodes are used to predict output data (aka response data) associated with
# the input data.
# The network has parameters controlling the combinations and transformations
# linking each layer to the next.(matrix W) These parameters are adjusted in
# order to make the input data best predict the output data according to some loss
# function, L. This is known as training the network.
# Stochastic Gradient Descent is an optimization approach that helps to maintain
# a good generalizing power, without getting too fixated on fitting any particular
# set of data. The idea is to repeatedly find the gradient of the loss function
# w.r.t. the parameters for small randomly chosen subsets of the training data,
# and to adjust the parameters by taking a step in the direction of the negative
# gradient.
start_time <- Sys.time()
# Set seed to make the results more stable
set.seed(2)
# netup() is the first function to use, a function to return a list of vector
# containing:
# h: a list of nodes for each layer. h[[l]] is a vector of length d[l] which
# contains the node values for layer l.
# W: a list of weight matrices W[[l]] is the weight matrix linking layer l to
# layer l+1, so W[[l]] is a d[l+1]*d[l] matrix and the length of W list will be
# L-1 (L is the number of total layers). Initialize the elements with U(0, 0.2)
# random deviates.
# b: a list of offset vectors. b[[l]] is the offset vector linking layer l to
# layer l+1, so the length of b[[l]] is equal to d[l+1] and there are L-1 vectors
# in list b (L is the maximum layer). Initialize the elements with U(0, 0.2)
# random deviates.
netup <- function(d){
# "lapply" applies function (length){return(rep(0,length))} to each element in
# d. For example, h[[i]] is a zero vector and length(h[[i]]) = d[i]
h <- lapply(d, function(length) rep(0,length))
W <- list()
b <- list()
# The number of layers
L <- length(d)
# There are only L-1 W matrices and L-1 b vectors
for (i in 1:(L-1)){
# Initialize the size of 'W' and W[[i]] is a d[i+1]*d[i] matrix
# Fill in the W matrices with uniformly distributed numbers within 0 and 0.2
W[[i]] <- matrix(runif(d[i] * d[i+1], 0, 0.2), d[i+1], d[i])
# Same idea to initialize b
b[[i]] <- runif(d[i+1], 0, 0.2)
}
# Return initialized network list
return(list("h" = h, "W" = W, "b" = b))
}
# forward() is the second function to use, compute the remaining node values
# implied by inp, and return the updated network list (as the only return object).
# nn: a network list as returned by netup
# inp: a vector of input values for the first layer.
# This forward function should
forward <- function(nn, inp){
# "netup_" marks network list returned by function netup
update_h <- nn$h
netup_W <- nn$W
netup_b <- nn$b
# Fill in updated h^1 with inp
update_h[[1]] <- inp
# The number of layers
netup_L <- length(update_h)
for(l in 1:(netup_L - 1)){
# For layer l, Whb = W %*% h + b. Whb is a vector.
# W[[l]] is the matrix linking l and l+1
# h[[l]] is a nodes vector
# b[[l]] is an offset vector
# with length(Whb^l) = length(h^(l+1))
Whb <- netup_W[[l]] %*% update_h[[l]] + netup_b[[l]]
# For h in layer l+1, h_j = max(Whb_j, 0)
for(i in 1:length(Whb)){
if(Whb[i] > 0){
update_h[[l+1]][i] <- Whb[i]
}
}
}
# Return updated network list
networklist <- list("h" = update_h, "W" = netup_W, "b" = netup_b)
return(networklist)
}
# Write a function backward(nn,k) for computing the derivatives of the loss
# corresponding to output class k for network nn (returned from forward function).
# Derivatives w.r.t. the nodes, weights and offsets should be computed and added
# to the network list as lists dh, dw, db.
# nn: network returned from forward function
# k: an integer representing class
backward<-function(nn, k){
# "f_" marks h, W and b returned from forward function.
# The length of lists h, W, b are the same as explained above for netup function
f_h <- nn$h ## length: f_L
f_W <- nn$W ## length: f_L - 1
f_b <- nn$b ## length: f_L - 1
f_L <- length(f_h) ## The number of layers
# Calculate the derivative of the loss k w.r.t. nodes on the last layer L
# Create an empty list having L sublists
dh <- vector("list", f_L)
# expj is a vector that expj[j] = exp(hL[j]), where f_h[[f_L]] = hL, the nodes
# vector on the last layer L
expj <- exp(f_h[[f_L]])
# Sum exp(hL[q]) for all q in hL
sumq <- sum(expj)
# dh_L[j] = exp(h_L)[j]/sum(h_L[q]), if j is not equal to k
dh[[f_L]] <- expj/sumq
# dh_L[k] = exp(h_L)[k]/sum(h_L[q])
dh[[f_L]][k] <- expj[k]/sumq - 1
# Compute derivatives of L w.r.t all other nodes by working backwards through
# the layers applying the chain rule (back-propagation)
db <- vector("list", f_L - 1) ## length: f_L - 1
dW <- vector("list", f_L - 1) ## length: f_L - 1
for (l in (f_L-1):1){
# For layer l
# If h(l+1)[j] is positive, d[j] = dh(l+1)[j]
d <- dh[[l+1]]
# If h(l+1)[j] is negative, d[j] = 0
d[which(f_h[[l+1]] < 0)] <- 0
dh[[l]] <- t(f_W[[l]]) %*% d
db[[l]] <- d
dW[[l]] <- d %*% t(f_h[[l]])
}
# Update the network lists
network = list("h" = f_h, "W" = f_W, "b" = f_b, "dh" = dh, "dW" = dW, "db" = db)
return (network)
}
# train function combined forward and backward functions to find the optimal
# value of W and b to get the best result for the current tasks and return the
# updated network list
# nn: a network list as returned by netup function
# inp: a vector of input values for the first layer.
# k: an integer representing class
# eta: a float representing step size
# mb: an integer representing the number of data to randomly sample
# nstep: an integer representing the the number of optimization steps
train <- function(nn, inp, k, eta = .01, mb = 10, nstep = 10000){
# The number of layers
L <- length(nn$h)
# Iterative nstep times to train W and b
for (istep in 1:nstep){
# Randomly choose mb rows as first layer values inp
s <- sample(nrow(inp), mb)
# Create empty lists sum_dW and sum_db to store the sum of derivatives dW, db
# There are L-1 sublists in sum_dW and sum_db
# Giving zeros to empty sublists
sum_dW <- sapply(vector("list", L-1), function(x) list(0))
sum_db <- sapply(vector("list", L-1), function(x) list(0))
# Iterative the randomly chose first layer inputs to get the different dW
# and db values, sum them up, then get the average values of dW and db
for (i in s){
# Use forward function to update all nodes in h list
f_nn <- forward(nn, as.numeric(inp[i,]))
# Use backward function to get dW and db
b_nn <- backward(f_nn, k[i])
# b_nn_dW and b_nn_db are used to store updated dW and db
b_nn_dW <- b_nn$dW
b_nn_db <- b_nn$db
for (j in 1:length(b_nn_db)){
sum_dW[[j]] <- sum_dW[[j]] + b_nn_dW[[j]]
sum_db[[j]] <- sum_db[[j]] + b_nn_db[[j]]
}
}
# new W = old W - eta*sum_dW/mb, where sum_dW/mb is the average value of dW
# new b = old b - eta*sum_dw/mb, where sum_db/mb is the average value of db
nn$W <- mapply(function(x, y) x - eta*(y/mb), nn$W, sum_dW, SIMPLIFY = FALSE)
nn$b <- mapply(function(x, y) x - eta*(y/mb), nn$b, sum_db, SIMPLIFY = FALSE)
}
return (nn)
}
# Use the dataset 'iris' from R to train a 4-8-7-3 network
data(iris)
# Find the species in 'iris'
class <- unique(iris[,"Species"])
# Use 1,2,3 to represent setosa, versicolor and virginica
iris[,"Species"] <- as.numeric(iris[,"Species"])
# Get row index for 'iris'
indices <- 1:nrow(iris)
# Select the test data by every 5 rows
iris_test <- iris[indices %% 5 == 0,]
# Except for test data, left of the dataset is training data
iris_train <- iris[indices %% 5 != 0,]
# Set the network structure and initialize it by netup function
layers <- c(4,8,7,3)
iris_nn <- netup(layers)
# Train the network based on training dataset
iris_nn <- train(iris_nn, iris_train[, 1:4], iris_train[, 5])
misclassification <- 0 ## The number of mis-classification
# Iterative rows to check the classification result produced by trained network model
for (i in 1:nrow(iris_test)){
# Compute the remaining node values implied by the first four elements
i_nn <- forward(iris_nn, as.numeric(iris_test[i, 1:4]))
# Find the maximum value in the last layer, the predicted class
i_class <- which.max(i_nn$h[[length(layers)]])
# If the predicted class is equal to the label, the predicted result is right
# Otherwise it is wrong
# Update mis-classification numbers
if (i_class != iris_test[i, 5]){
misclassification <- misclassification + 1
}
}
end_time <- Sys.time()
print(end_time - start_time)
# Calculate the mis-classification rate (i.e. the proportion mis-classified) for
# the test set.
misclassification_rate <- misclassification/nrow(iris_test)
cat("mis-classification rate: ", misclassification_rate)
